大约三天读完，这本书浅显易懂，非常适合入门，只有权重调整值的推导过程看得不是很透彻。
## 基本概念
神经网络也是机器学习的一种实现，可以应用在有监督学习和无监督学习，因为中间可以有较多层，所以属于深度学习方法。

神经网络的名字很唬人，其实概念挺朴素的，是由含一个输入层一个输出层和若干隐藏层构成的有向无环图（这名字也唬人），看图像一目了然，为啥叫隐藏层呢，就是因为和输入输出没关系，看不见，有点儿神秘。每层的每个结点借助生物的概念称为神经元，各层之间神经元相互链接。

![](http://ww1.sinaimg.cn/mw690/780b940aly1frpn9bdowqj20un0u0tai.jpg)

算法训练包含两个阶段：输入向输出传送叫前向馈送信号，输出向输入传送叫反向误差传播。把输入前馈计算得到输出，把输出与目标值比对计算误差，把误差反向传播修正链接权重。具体过程是：

1. “输入层与隐藏层之间的链接权重”与“输入信号”加权求和，“和值”通过神经元函数（通常是阶跃函数）运算得到隐藏层的结果。
2. 用与第一步相同的过程计算出输出层的结果。
3. 目标值-输出值=误差。
4. 将误差按权重反向传播给隐藏层。
5. 用梯度下降法最小化误差，计算出误差调整值，初始误差+误差调整值=训练结果。

### 注意点
- 初始权重未知，为了避免落入错误的山谷，随机选取多个起始点（初始权重）。

- 根据调整应用在信号上的神经元函数的斜率来调整权重。

- 梯度下降法最小化误差函数。

- 训练过程就是调整权重的过程，初始权重的设定要注意避免网络饱和。初始权重过大容易导致网络饱和，初始权重为0或者相等将导致丧失学习能力。

- 输入信号通常取值范围是0.01 ~ 0.99或-1.0 ~ 1.0，一个比较合适的输出取值范围0.01 ~ 0.99。

## 问题
- 1.12节反向传播误差到更多层中，最后一张图将误差传播到了输入层，这给我造成了困惑，想了大半天，因为在后面调整误差的时候只用到了隐藏层和输出层的误差，其实在三层的网络中，只需要用输出层误差计算Who和用隐藏层误差计算Wih，计算输入层的误差其实没有用，书中应该是借用这一步推导更明确一下传播误差的方法。

- 2.4.4节wih初始化时正态分布的标准差取1/sqrt(传入链接数目)，代码中隐藏层传入链接数目用hnodes，输出层传入链接数目用onodes，我认为传入链接数目应该是上一层的结点数，所以分别应该是inodes和hnodes。

- 训练时cpu使用率40%，跑了一会以后升高到60%，临近计算结束又降到40%，不是一直跑满。

## 训练结果
MNIST手写数字识别

世代 | 隐藏层 | 隐藏层结点数 | 学习率 | 识别率 | 训练时长(s)
---|---|---|---|---|---
1 | 1 | 100 | 0.1 | 0.9479 | 24.0102
1 | 1 | 100 | 0.2 | **0.9518** | 25.4991
1 | 1 | 100 | 0.3 | 0.9443 | 26.0787
1 | 1 | 100 | 0.6 | 0.9209 | 24.9883
1 | 1 | 10 | 0.2 | **0.8495** | 11.4656
1 | 1 | 200 | 0.2 | 0.9558 | 97.4185
2 | 1 | 200 | 0.2 | 0.9574 | 49.0803
3 | 1 | 200 | 0.2 | 0.9602 | 78.7243
4 | 1 | 200 | 0.2 | 0.959 | 104.7202
5 | 1 | 200 | 0.2 | **0.9627** |146.9687
6 | 1 | 200 | 0.2 | 0.9577 | 162.4879